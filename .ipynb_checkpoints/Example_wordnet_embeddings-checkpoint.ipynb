{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a meant to produce a visualization of NLP methods using the following lexical ressources:\n",
    "\n",
    "- Bert embeddings\n",
    "- WordNet knowledge base\n",
    "\n",
    "The idea is to find full-sentence input containing words that pertain to distinct categories, to obtain BERT embeddings of these sentences, and then to project the embeddings onto a 2-dimensional space \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "## import packages for general NLP\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag, map_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from string import punctuation\n",
    "from ast import literal_eval\n",
    "import sys\n",
    "\n",
    "# embeddings\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "## visuals\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find sentence realizations containing lemmas pertaining to distinct categories of hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_pos(treebank_tag):\n",
    "    \"\"\"Method adapted from various stackoverflow posts\n",
    "    \n",
    "    https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    \n",
    "    \n",
    "    :param treebank_tag: the string of the english ptb pos tag\n",
    "    \n",
    "    :return:\n",
    "      the wordnet postag object \n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def lemmatize_sent(sent):\n",
    "    \"\"\"Function returning lemmas and tokens of sentence via wordnet lemmatizer\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return [(LEMMATIZER.lemmatize(tok.lower(), pos = wn_pos(pos)), tok) \n",
    "            if wn_pos(pos) and tok not in punctuation else ('', tok)\n",
    "           for tok, pos in pos_tag(word_tokenize(sent))]\n",
    "\n",
    "\n",
    "def get_hyponyms(hyper_syns, lemma_names, hypos):\n",
    "    \"\"\"Recursive method to extend list of lemmas of hyponyms from the given hypernym\n",
    "    \n",
    "    :params: \n",
    "        hyper_syns: wordnet synset object of the hypernym\n",
    "        lemma_names: list of lemmas to be extended\n",
    "        hypos: list of hyponyms to be extended    \n",
    "    \n",
    "    \"\"\"\n",
    "    for hypo in hyper_syns.hyponyms():\n",
    "        hypos.append(hypo)\n",
    "        lemma_names.extend(hypo.lemma_names())\n",
    "        get_hyponyms(hypo, lemma_names, hypos)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_sents():\n",
    "    \"\"\"Method returning sentences which contain lemmas pertaining to distinct categories of hypernyms\n",
    "    \n",
    "    :return:\n",
    "        sentences_to_encode: a dictionary with the sentence, the hyponyms index within the sentence, and the hypernym category\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sentences_to_encode = dict()\n",
    "    hypers_to_plot = ['diversion.n.01', 'wrongdoing.n.02', 'mistreatment.n.01', 'travel.n.01', 'vehicle.n.01']\n",
    "    category2index = {'diversion.n.01' : 1, \n",
    "                      'wrongdoing.n.02': 2, \n",
    "                      'mistreatment.n.01': 2, \n",
    "                      'travel.n.01': 3, \n",
    "                      'vehicle.n.01': 4\n",
    "                     }\n",
    "    for hyper in hypers_to_plot:\n",
    "        syns = wn.synset(hyper)\n",
    "        lemma_names = []\n",
    "        hypos = []\n",
    "        get_hyponyms(syns, lemma_names, hypos) \n",
    "        for hypo in hypos:\n",
    "            examples = hypo.examples()\n",
    "            if examples:\n",
    "                for example in examples:\n",
    "                    if example:\n",
    "                        lemma_tokenized = lemmatize_sent(example)\n",
    "                        lemmatized_sent = [x[0] for x in lemma_tokenized]\n",
    "                        tokenized_sent = [x[1] for x in lemma_tokenized]\n",
    "                        for lemma in lemma_names:\n",
    "                            if lemma in lemmatized_sent:\n",
    "                                sentences_to_encode[lemma] = {\n",
    "                                    \"sentence\" : ' '.join(tokenized_sent),\n",
    "                                    \"position\" : lemmatized_sent.index(lemma),\n",
    "                                    \"hyper_cat\" : category2index[hyper]\n",
    "                                }\n",
    "    return sentences_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sents()\n",
    "print(\"The above method succeeded in collecting \", len(sentences), \"sentences from Wordnet.\", '\\n\\n')\n",
    "\n",
    "print(\"An example of the data collected is as follows:\")\n",
    "sentences['sport']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed sentences with indexed highlighted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for sent in sentences:\n",
    "        input_ids = torch.tensor(tokenizer.encode(sentences[sent]['sentence'])).unsqueeze(0) \n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0].squeeze()\n",
    "        embed = last_hidden_states.detach().tolist()\n",
    "        labels = np.zeros(len(embed))\n",
    "        toks = [tokenizer.convert_ids_to_tokens(idx) for idx in input_ids]\n",
    "        if sentences[sent]['sentence'].split()[sentences[sent]['position']] in toks[0]:\n",
    "            new_index = toks[0].index(sentences[sent]['sentence'].split()[sentences[sent]['position']])\n",
    "            labels[new_index] = sentences[sent]['hyper_cat']\n",
    "        all_embed.extend(embed)\n",
    "        all_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2category = {1: \"sports\", 2: \"wrong_doing\", 3: \"travel\", 4: \"vehicle\"}\n",
    "RS = 25111993\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "tnse = TSNE(random_state = RS, n_components =2).fit_transform(all_embed)\n",
    "x_0 = [x[0] for x, l in zip(tnse, all_labels) if not l]\n",
    "y_0 = [x[1] for x, l in zip(tnse, all_labels) if not l]\n",
    "all_labels_0 = [x for x in all_labels if not x]\n",
    "ax = sns.scatterplot(x_0, y_0, hue = all_labels_0, alpha = 0.1, legend = \"full\", palette = sns.color_palette(n_colors = 1))\n",
    "x_123 = [x[0] for x, l in zip(tnse, all_labels) if l]\n",
    "y_123 = [x[1] for x, l in zip(tnse, all_labels) if l]\n",
    "all_labels_123 = [index2category[x] for x in all_labels if x]\n",
    "ax = sns.scatterplot(x_123, y_123, hue = all_labels_123, palette = sns.color_palette(\"Dark2\", n_colors=len([x for x in index2category])),legend = \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
